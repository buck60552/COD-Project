{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buck60552/COD-Project/blob/main/Senior_Seminar_Proj_Build_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l5-S4Q1A6e4"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzDOFsTbyQvC",
        "outputId": "a35f746f-af77-4f77-c316-fd0c128552a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf version: 2.15.0\n",
            "np version: 1.25.2\n",
            "cv2 version: 4.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.ops.gen_logging_ops import Print\n",
        "from tensorflow import keras\n",
        "from keras.layers import (Dense, Conv2D, Conv2DTranspose, Dropout, Input,\n",
        "                          MaxPooling2D, concatenate, UpSampling2D)\n",
        "from keras.models import (Sequential, Model, load_model)\n",
        "from keras import backend\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv\n",
        "import random\n",
        "import glob\n",
        "import PIL\n",
        "import re\n",
        "import os\n",
        "\n",
        "# prints current library version\n",
        "print(\"tf version: \"+tf.__version__)\n",
        "print(\"np version: \"+np.__version__)\n",
        "print(\"cv2 version: \"+cv.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "5GYxq6nTCobQ",
        "outputId": "601a951e-8894-443b-e8e3-6149d4da9ebb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "BaseException",
          "evalue": "ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5562d36dde60>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TPU detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running on TPU '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'worker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tpu, zone, project, job_name, coordinator_name, coordinator_address, credentials, service, discovery_url)\u001b[0m\n\u001b[1;32m    234\u001b[0m       \u001b[0;31m# Default Cloud environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m       self._cloud_tpu_client = client.Client(\n\u001b[0m\u001b[1;32m    236\u001b[0m           \u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/tpu/client/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tpu, zone, project, credentials, service, discovery_url)\u001b[0m\n\u001b[1;32m    158\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please provide a TPU Name to connect to.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Please provide a TPU Name to connect to.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mBaseException\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5562d36dde60>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running on TPU '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'worker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_connect_to_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBaseException\u001b[0m: ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!"
          ]
        }
      ],
      "source": [
        "# detect and utilise Colab's TPU\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzzdi_LH6str"
      },
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyommKjA6tF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787c256c-6e17-494a-8d6e-36d26136150f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training length: 748\n",
            "testing length: 330\n"
          ]
        }
      ],
      "source": [
        "# natural sort algorithm to keep images paired with the correct ground truth\n",
        "def nat_sort(list: list[str]) :\n",
        "  def alphaKey(key) :\n",
        "    return [int(s) if s.isdigit() else s.lower() for s in re.split(\"([0-9]+)\", key)]\n",
        "  return sorted(list, key=alphaKey)\n",
        "\n",
        "\n",
        "# paths to folders of iamges and their ground truths\n",
        "# dataset 1 contains people, dataset 2 contains animals\n",
        "# unused dataset commented out to improve performance\n",
        "dataset1_path_train_images =       \"/content/drive/MyDrive/Work/Computer_Science/CSCI_599/Proj/archive (1)/dataset-splitM/Training/images\"\n",
        "dataset1_path_train_Ground_Truth = \"/content/drive/MyDrive/Work/Computer_Science/CSCI_599/Proj/archive (1)/dataset-splitM/Training/GT\"\n",
        "\n",
        "#dataset2_path_train_images = \"/content/drive/MyDrive/Computer_Science/CSCI_599/Proj/Animals data/COD10K-v2/Train/Images/Image\"\n",
        "#dataset2_path_train_Ground_Truth = \"/content/drive/MyDrive/Computer_Science/CSCI_599/Proj/Animals data/COD10K-v2/Train/GT_Objects/GT_Object\"\n",
        "\n",
        "\n",
        "dataset1_path_test_images = \"/content/drive/MyDrive/Work/Computer_Science/CSCI_599/Proj/archive (1)/dataset-splitM/Testing/images\"\n",
        "dataset1_path_test_Ground_Truth = \"/content/drive/MyDrive/Work/Computer_Science/CSCI_599/Proj/archive (1)/dataset-splitM/Testing/GT\"\n",
        "\n",
        "#dataset2_path_test_images = \"/content/drive/MyDrive/Computer_Science/CSCI_599/Proj/Animals data/COD10K-v2/Test/Images/Image\"\n",
        "#dataset2_path_test_Ground_Truth = \"/content/drive/MyDrive/Computer_Science/CSCI_599/Proj/Animals data/COD10K-v2/Test/GT_Objects/GT_Object\"\n",
        "\n",
        "\n",
        "# lists of paths to images and gts natural sorted to ensure indices lead to corect image gt pairs later\n",
        "# dataset 1 contains people, dataset 2 contains animals\n",
        "Train_im_paths1 = np.array(nat_sort(glob.glob(f\"{dataset1_path_train_images}/*.jpg\")))\n",
        "Train_GT_paths1 = np.array(nat_sort(glob.glob(f\"{dataset1_path_train_Ground_Truth}/*.jpg\")))\n",
        "\n",
        "#Train_im_paths2 = np.array(nat_sort(glob.glob(f\"{dataset2_path_train_images}/*.jpg\")))\n",
        "#Train_GT_paths2 = np.array(nat_sort(glob.glob(f\"{dataset2_path_train_Ground_Truth}/*.png\")))\n",
        "\n",
        "\n",
        "test_im_paths1  = np.array(nat_sort(glob.glob(f\"{dataset1_path_test_images}/*.jpg\")))\n",
        "test_GT_paths1  = np.array(nat_sort(glob.glob(f\"{dataset1_path_test_Ground_Truth}/*.png\")))\n",
        "\n",
        "#test_im_paths2  = np.array(nat_sort(glob.glob(f\"{dataset2_path_test_images}/*.jpg\")))\n",
        "#test_GT_paths2  = np.array(nat_sort(glob.glob(f\"{dataset2_path_test_Ground_Truth}/*.png\")))\n",
        "\n",
        "\n",
        "# function : takes in folder paths, and a maximum value.\n",
        "# then scans for files ending in .jpg or .png and loads them into numpy arrays\n",
        "# thirdly it preprocesses the Ground truths\n",
        "# process continues until an arbuitrary maximum number of files is reached and terminates\n",
        "def Load_pair(im_path, GT_path, max) :\n",
        "  tru_images = []\n",
        "  tru_GTs = []\n",
        "  count1 = 0\n",
        "  count2 = 0\n",
        "  for fname in im_path :\n",
        "    if count1 >= max :\n",
        "      break\n",
        "    if fname.endswith('.jpg') or fname.endswith('.png') :\n",
        "      count1 += 1\n",
        "      image = cv.imread(fname)\n",
        "      image = cv.resize(image, (256,256))\n",
        "      tru_images.append(image)\n",
        "      print(f\"\\rImage Count: {count1}/{count2}/{max}\", end = \"\")\n",
        "  for fname in GT_path :\n",
        "    if count2 >= max :\n",
        "      break\n",
        "    if fname.endswith('.jpg') or fname.endswith('.png') :\n",
        "      count2 += 1\n",
        "      GT = cv.imread(fname)\n",
        "      GT = cv.resize(GT, (256,256))\n",
        "      GT = cv.normalize(GT, None, 0, 1, cv.NORM_MINMAX, dtype=cv.CV_32F)\n",
        "      tru_GTs.append(GT)\n",
        "      print(f\"\\rImage/GT Count: {count1}/{count2}/{max}\", end = \"\")\n",
        "  print(\" | Load Complete\")\n",
        "  return np.array(tru_images), np.array(tru_GTs)\n",
        "\n",
        "print(f\"training length: {len(Train_im_paths1)}\")\n",
        "print(f\"testing length: {len(test_im_paths1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwZQ1NY0UPdC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx27QwFiionD"
      },
      "outputs": [],
      "source": [
        "# runs the above methods and prints the shape of the first element of the lists\n",
        "# to ensure that they are correct\n",
        "\n",
        "train_im, train_GT = Load_pair(Train_im_paths1, Train_GT_paths1, 748)\n",
        "print(f\"Training IM Shape : {train_im[0].shape}\")\n",
        "print(f\"Training GT Shape : {train_GT[0].shape}\")\n",
        "print()\n",
        "test_im, test_gt = Load_pair(test_im_paths1, test_GT_paths1, 330)\n",
        "print(f\"Validation IM Shape : {test_im[0].shape}\")\n",
        "print(f\"Validation GT Shape : {test_gt[0].shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GQWZtZvkTNC"
      },
      "source": [
        "##Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEm-ouIvkYt0"
      },
      "outputs": [],
      "source": [
        "#generate a random number within the range of the training set\n",
        "image_number = random.randint(0, len(train_im))\n",
        "\n",
        "# prints the greatest value within the GT\n",
        "# this serves to validate that the normalization procedure was a success\n",
        "print(np.amax(train_GT[image_number]))\n",
        "\n",
        "# generates a GT image pair and verifies that things are being read correctly\n",
        "# also validates that the order of images and GTs match.\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plt.imshow(np.reshape(train_im[image_number], (256, 256)), cmap='gray')\n",
        "plt.subplot(122)\n",
        "plt.imshow(np.reshape(train_GT[image_number], (256, 256)), cmap='gray')\n",
        "plt.show\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbGJBInkBA5D"
      },
      "source": [
        "Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71svwVeVBB73"
      },
      "outputs": [],
      "source": [
        "backend.clear_session()\n",
        "\n",
        "def U_Net(input_shape=(256,256,3)):\n",
        "  # input | layer 0\n",
        "  inputs = Input(input_shape)\n",
        "\n",
        "  # encode 1 | layer 1\n",
        "  convolution2_1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "  convolution2_1 = Conv2D(64, 3, activation='relu', padding='same')(convolution2_1)\n",
        "  pooling2_1 = MaxPooling2D(pool_size=(2,2))(convolution2_1)\n",
        "\n",
        "  # encode 2 | layer 2\n",
        "  convolution2_2 = Conv2D(128, 3, activation='relu', padding='same')(pooling2_1)\n",
        "  convolution2_2 = Conv2D(128, 3, activation='relu', padding='same')(convolution2_2)\n",
        "  pooling2_2 = MaxPooling2D(pool_size=(2,2))(convolution2_2)\n",
        "\n",
        "  # encode 3 |  layer 3\n",
        "  convolution2_3 = Conv2D(256, 3, activation='relu', padding='same')(pooling2_2)\n",
        "  convolution2_3 = Conv2D(256, 3, activation='relu', padding='same')(convolution2_3)\n",
        "  pooling2_3 = MaxPooling2D(pool_size=(2,2))(convolution2_3)\n",
        "\n",
        "  # encode 4 | layer 4\n",
        "  convolution2_4 = Conv2D(512, 3, activation='relu', padding='same')(pooling2_3)\n",
        "  convolution2_4 = Conv2D(512, 3, activation='relu', padding='same')(convolution2_4)\n",
        "  pooling2_4 = MaxPooling2D(pool_size=(2,2))(convolution2_4)\n",
        "\n",
        "  # base | layer 5\n",
        "  base2 = Conv2D(1024, 3 , activation = 'relu', padding = 'same')(pooling2_4)\n",
        "  base2 = Conv2D(1024, 3 , activation = 'relu', padding = 'same')(base2)\n",
        "\n",
        "  # decode 1 | layer 6\n",
        "  upsample2_5 = UpSampling2D(size=(2,2))(base2)\n",
        "  convolution2_5 = Conv2D(512, 2, activation='relu', padding='same')(upsample2_5)\n",
        "  merge2_5 = concatenate([convolution2_4, convolution2_5], axis =3)\n",
        "  convolution2_5 = Conv2D(512, 3, activation='relu', padding='same')(merge2_5)\n",
        "  convolution2_5 = Conv2D(512, 3, activation='relu', padding='same')(convolution2_5)\n",
        "\n",
        "  # decode 2 | layer 7\n",
        "  upsample2_6 = UpSampling2D(size=(2,2))(convolution2_5)\n",
        "  convolution2_6 = Conv2D(256, 2, activation='relu', padding='same')(upsample2_6)\n",
        "  merge2_6 = concatenate([convolution2_3, convolution2_6], axis =3)\n",
        "  convolution2_6 = Conv2D(256, 3, activation='relu', padding='same')(merge2_6)\n",
        "  convolution2_6 = Conv2D(256, 3, activation='relu', padding='same')(convolution2_6)\n",
        "\n",
        "  # decode 3 | layer 8\n",
        "  upsample2_7 = UpSampling2D(size=(2,2))(convolution2_6)\n",
        "  convolution2_7 = Conv2D(128, 2, activation='relu', padding='same')(upsample2_7)\n",
        "  merge2_7 = concatenate([convolution2_2, convolution2_7], axis =3)\n",
        "  convolution2_7 = Conv2D(128, 3, activation='relu', padding='same')(merge2_7)\n",
        "  convolution2_7 = Conv2D(128, 3, activation='relu', padding='same')(convolution2_7)\n",
        "\n",
        "  # decode 4 | layer 9\n",
        "  upsample2_8 = UpSampling2D(size=(2,2))(convolution2_7)\n",
        "  convolution2_8 = Conv2D(64, 2, activation='relu', padding='same')(upsample2_8)\n",
        "  merge2_8 = concatenate([convolution2_1, convolution2_8], axis =3)\n",
        "  convolution2_8 = Conv2D(64, 3, activation='relu', padding='same')(merge2_8)\n",
        "  convolution2_8 = Conv2D(64, 3, activation='relu', padding='same')(convolution2_8)\n",
        "\n",
        "\n",
        "  # output | layer 10\n",
        "  outputs = Conv2D(3, 1, padding='same', activation='softmax')(convolution2_8)\n",
        "\n",
        "  # model defined\n",
        "  U_Net_Model = Model(inputs=inputs, outputs=outputs, name='U-Net')\n",
        "\n",
        "  return U_Net_Model\n",
        "\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "  input_shape = (256,256,3)\n",
        "  generator2 = U_Net(input_shape)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "  loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "  generator2.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
        "  generator2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzPNkqL_39Ta"
      },
      "source": [
        "Model 2 Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVmNpe1ahg-M"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(generator2, show_shapes=True, show_layer_names=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKZOHFQG2zCh"
      },
      "source": [
        "#EXECUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFotQQy4wVGH"
      },
      "source": [
        "model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG-7tqzC9sCM"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "batch = 64\n",
        "\n",
        "model = U_Net(input_shape=(256,256,3))\n",
        "\n",
        "run_U_net_model = generator2.fit(train_im, train_GT,\n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=batch,\n",
        "                                 validation_data=(test_im, test_gt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBVENUcEwZZe"
      },
      "source": [
        "Model 2 data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFwUwQHFxmw4"
      },
      "source": [
        "Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD7e3fIpQkCO"
      },
      "outputs": [],
      "source": [
        "# run_U_net_model.save('/content/drive/MyDrive/Computer_Science/CSCI_599/Proj/model_1')\n",
        "\n",
        "loss = run_U_net_model.history['loss']\n",
        "val_loss = run_U_net_model.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(np.reshape(test_im[190], (256,256, 3)), cmap='gray')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(np.reshape(test_gt[190], (256,256, 3)), cmap='gray')\n",
        "plt.title('Actual Mask')\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(PIL.Image.fromarray(predict_threshold[0,:,:,0]), cmap='gray')\n",
        "plt.title('Predicted Mask')\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhDemAb2xoXN"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI1SJxoOvsYe"
      },
      "outputs": [],
      "source": [
        "accuracy = run_U_net_model.history['accuracy']\n",
        "val_accuracy = run_U_net_model.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, accuracy, 'y', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHfuMlWLxsK8"
      },
      "source": [
        "IoU and Test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLy3Tqu6x7Cs"
      },
      "outputs": [],
      "source": [
        "# IOU\n",
        "input_num = random.randint(0,len(test_im)-1)\n",
        "\n",
        "input_image = test_im[190]\n",
        "input_image = np.expand_dims(input_image, axis=0)\n",
        "print(input_image.shape)\n",
        "\n",
        "predict1 = generator2.predict(input_image)\n",
        "predict_threshold = (predict1 > 0.5)\n",
        "\n",
        "intersection = np.logical_and(predict_threshold, test_gt)\n",
        "union = np.logical_or(predict_threshold, test_gt)\n",
        "iou_score = np.sum(intersection) / np.sum(union)\n",
        "print(f\"image number: {input_num}\")\n",
        "print(f\"IoU score: {iou_score*100}\")\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(np.reshape(test_im[190], (256,256, 3)), cmap='gray')\n",
        "plt.title('Original Image')\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(np.reshape(test_gt[190], (256,256, 3)), cmap='gray')\n",
        "plt.title('Actual Mask')\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(PIL.Image.fromarray(predict_threshold[0,:,:,0]), cmap='gray')\n",
        "plt.title('Predicted Mask')\n",
        "plt.show"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "mount_file_id": "16JYuprd96HbpAEWxX3m1FXE4DSrC_s4J",
      "authorship_tag": "ABX9TyP2lPTWhtgVM9Q7hUG+jfsN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}